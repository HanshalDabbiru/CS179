import kagglehub
import pandas as pd
import os
import pyGMs as gm
import numpy as np
import pandas as pd
import numpy as np
import torch
import pyro
import pyro.distributions as dist
from pyro.infer import SVI, Trace_ELBO
from pyro.optim import Adam
import argparse


# Download latest version
path = kagglehub.dataset_download("drgilermo/nba-players-stats")
print("Path to dataset files:", path)


df = pd.read_csv(os.path.join(path, "Seasons_Stats.csv"), encoding='latin1')





df.head(10)


df['Player'] = df['Player'].str.replace('*', '', regex=False)
df = df.drop(columns=["Unnamed: 0"])
df = df.drop(columns=["blanl", "blank2"])
df = df[df['Year'].notna()]
df['Year'] = df['Year'].astype(int)
df = df[df['Year'] > 1973]
df = df.drop(columns=['GS', 'MP', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS/48', 'OBPM', 'DBPM', '3P', '3PA', '3P%', 'eFG%', 'ORB', 'DRB', 'PF', 'TOV'])


df.head(10)


nan_count = np.sum(df.isnull(), axis=0)
print(nan_count)
print(df.shape)





mvp_winners = {
    1956: "Bob Pettit",
    1957: "Bob Cousy",
    1958: "Bill Russell",
    1959: "Bob Pettit",
    1960: "Wilt Chamberlain",
    1961: "Bill Russell",
    1962: "Bill Russell",
    1963: "Bill Russell",
    1964: "Oscar Robertson",
    1965: "Bill Russell",
    1966: "Wilt Chamberlain",
    1967: "Wilt Chamberlain",
    1968: "Wilt Chamberlain",
    1969: "Wes Unseld",
    1970: "Willis Reed",
    1971: "Kareem Abdul-Jabbar",
    1972: "Kareem Abdul-Jabbar",
    1973: "Dave Cowens",
    1974: "Kareem Abdul-Jabbar",
    1975: "Bob McAdoo",
    1976: "Kareem Abdul-Jabbar",
    1977: "Kareem Abdul-Jabbar",
    1978: "Bill Walton",
    1979: "Moses Malone",
    1980: "Kareem Abdul-Jabbar",
    1981: "Julius Erving",
    1982: "Moses Malone",
    1983: "Moses Malone",
    1984: "Larry Bird",
    1985: "Larry Bird",
    1986: "Larry Bird",
    1987: "Magic Johnson",
    1988: "Michael Jordan",
    1989: "Magic Johnson",
    1990: "Magic Johnson",
    1991: "Michael Jordan",
    1992: "Michael Jordan",
    1993: "Charles Barkley",
    1994: "Hakeem Olajuwon",
    1995: "David Robinson",
    1996: "Michael Jordan",
    1997: "Karl Malone",
    1998: "Michael Jordan",
    1999: "Karl Malone",
    2000: "Shaquille O'Neal",
    2001: "Allen Iverson",
    2002: "Tim Duncan",
    2003: "Tim Duncan",
    2004: "Kevin Garnett",
    2005: "Steve Nash",
    2006: "Steve Nash",
    2007: "Dirk Nowitzki",
    2008: "Kobe Bryant",
    2009: "LeBron James",
    2010: "LeBron James",
    2011: "Derrick Rose",
    2012: "LeBron James",
    2013: "LeBron James",
    2014: "Kevin Durant",
    2015: "Stephen Curry",
    2016: "Stephen Curry",
    2017: "Russell Westbrook",
    2018: "James Harden"
}


df['Won_MVP'] = df.apply(lambda row: int(row['Player'] == mvp_winners.get(row['Year'])), axis=1)
df.head()


print(list(df.columns))


df_top_scorers = df.sort_values(['Year', 'PTS'], ascending=[True, False]).groupby('Year').head(5)
df = pd.concat([
    df_top_scorers,
    df[df['Won_MVP'] == 1]
]).drop_duplicates().reset_index(drop=True)


from pandas.api.types import is_numeric_dtype, is_object_dtype
from scipy.special import logsumexp

# -------------------- Preprocessing --------------------

def load_and_preprocess_from_df(df):
    df = df.copy()
    df = df.dropna(subset=['Won_MVP'])

    y = df['Won_MVP'].astype(int).values

    exclude_cols = ['Player', 'Tm', 'Won_MVP', 'G']
    features = [col for col in df.columns if col not in exclude_cols]

    # Fill NaNs with median for numerical columns
    for col in features:
        if df[col].isnull().any():
            median = df[col].median()
            df[col] = df[col].fillna(median)

    # Use fewer bins (3 instead of 5) to reduce overfitting
    bins_dict = {}
    X = []
    for col in features:
        if is_object_dtype(df[col].dtype):
            continue
        # Use 3 bins instead of 5 to reduce model complexity
        binned, bins = pd.qcut(df[col], q=10, labels=False, retbins=True, duplicates='drop')
        X.append(binned.astype(int).values)
        bins_dict[col] = bins

    X = np.stack(X, axis=1)
    return X, y, features, bins_dict

# -------------------- Pyro Naive Bayes Model with Smoothing --------------------

def build_model(feature_bins, num_classes=2, alpha=1.0):
    def model(X, y=None):
        # Add Laplace smoothing to class priors
        class_probs = pyro.param("class_probs", 
                                 torch.ones(num_classes) / num_classes,
                                 constraint=dist.constraints.simplex)

        with pyro.plate("data", X.shape[0]):
            y_hat = pyro.sample("y", dist.Categorical(class_probs), obs=y)
            for i, bins in enumerate(feature_bins):
                # Add Laplace smoothing to feature probabilities
                # Initialize with uniform + smoothing
                init_probs = torch.ones(num_classes, bins) / bins + alpha / bins
                probs = pyro.param(f"feature_{i}_probs", 
                                  init_probs,
                                  constraint=dist.constraints.simplex)
                pyro.sample(f"feature_{i}", dist.Categorical(probs[y_hat]), obs=X[:, i])
    return model

def guide(X, y=None):
    pass  # No guide needed for Naive Bayes

# -------------------- Training --------------------

def train(X, y, feature_bins, alpha=1.0):
    pyro.clear_param_store()
    
    print(f"Training data shape: {X.shape}")
    print(f"Class distribution: {np.bincount(y)}")
    print(f"MVP rate: {y.mean():.4f}")
    print(f"Feature bins: {feature_bins}")

    model = build_model(feature_bins, alpha=alpha)
    # Use lower learning rate for more stable training
    svi = SVI(model, guide, Adam({"lr": 0.001}), loss=Trace_ELBO())

    X_tensor = torch.tensor(X, dtype=torch.long)
    y_tensor = torch.tensor(y, dtype=torch.long)

    for step in range(3000):  # More training steps
        loss = svi.step(X_tensor, y_tensor)
        if step % 100 == 0:
            print(f"[{step}] Loss: {loss:.2f}")
            
            # Debug: Check parameter values
            class_probs = pyro.param("class_probs").detach().numpy()
            print(f"  Class probs: {class_probs}")

# -------------------- Prediction with Better Numerical Stability --------------------

def predict_proba_stable(X_new, feature_bins):
    """Predict probabilities with better numerical stability."""
    class_probs = pyro.param("class_probs").detach().numpy()
    
    # Add small epsilon to prevent log(0)
    eps = 1e-10
    class_probs = np.clip(class_probs, eps, 1.0)
    
    preds_proba = []
    
    for row_idx, row in enumerate(X_new):
        # Start with log class priors
        logp = np.log(class_probs)
        
        # Add log likelihoods for each feature
        for i, val in enumerate(row):
            if val >= len(pyro.param(f"feature_{i}_probs").detach().numpy()[0]):
                # Handle out-of-range values by using last bin
                val = len(pyro.param(f"feature_{i}_probs").detach().numpy()[0]) - 1
                
            feature_probs = pyro.param(f"feature_{i}_probs").detach().numpy()
            feature_probs = np.clip(feature_probs, eps, 1.0)
            logp += np.log(feature_probs[:, val])
        
        # Use scipy's logsumexp for numerical stability
        log_proba_normalized = logp - logsumexp(logp)
        proba = np.exp(log_proba_normalized)
        
        # Ensure probabilities sum to 1
        proba = proba / proba.sum()
        
        preds_proba.append(proba[1])  # Probability of being MVP
        
        # Debug first few predictions
        if row_idx < 3:
            print(f"Row {row_idx}: logp={logp}, proba={proba}, MVP_prob={proba[1]:.6f}")
    
    return np.array(preds_proba)

# -------------------- Alternative: Use Sklearn for Comparison --------------------

def train_sklearn_baseline(X, y):
    """Train sklearn Naive Bayes for comparison."""
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.metrics import accuracy_score, classification_report
    
    print("\n--- Sklearn Baseline ---")
    nb = MultinomialNB(alpha=1.0)  # Laplace smoothing
    nb.fit(X, y)
    
    # Get probabilities
    probs = nb.predict_proba(X)[:, 1]  # Probability of MVP class
    
    print(f"Sklearn MVP probabilities range: {probs.min():.6f} to {probs.max():.6f}")
    print(f"Sklearn MVP probabilities mean: {probs.mean():.6f}")
    print(f"Sklearn MVP probabilities std: {probs.std():.6f}")
    
    return probs

# -------------------- Main Execution --------------------

from sklearn.model_selection import train_test_split

def run_pipeline_from_df(df, use_sklearn_comparison=False, test_frac=0.2, seed=42):
    # Split by year
    unique_years = sorted(df['Year'].unique())
    train_years, test_years = train_test_split(unique_years, test_size=test_frac, random_state=seed)

    # Split df
    df_train = df[df['Year'].isin(train_years)].copy()
    df_test = df[df['Year'].isin(test_years)].copy()

    # Preprocess training data
    X_train, y_train, features, bins_dict = load_and_preprocess_from_df(df_train)
    feature_bins = [len(bins_dict[col]) - 1 for col in features if is_numeric_dtype(df[col])]

    print("=== Training Pyro Model ===")
    train(X_train, y_train, feature_bins, alpha=5.0)

    print("\n=== Evaluating Pyro Model ===")
    # Preprocess test data using same binning
    X_test = []
    for col in features:
        if is_object_dtype(df_test[col].dtype):
            continue
        bins = bins_dict[col]
        binned = pd.cut(df_test[col], bins=bins, labels=False, include_lowest=True)
        binned = binned.fillna(0).astype(int)
        X_test.append(binned.values)
    X_test = np.stack(X_test, axis=1)
    y_test = df_test['Won_MVP'].astype(int).values

    probs_pyro = predict_proba_stable(X_test, feature_bins)

    print(f"Pyro MVP probabilities range: {probs_pyro.min():.6f} to {probs_pyro.max():.6f}")
    print(f"Pyro MVP probabilities mean: {probs_pyro.mean():.6f}")
    print(f"Pyro MVP probabilities std: {probs_pyro.std():.6f}")

    # Compare with sklearn
    if use_sklearn_comparison:
        probs_sklearn = train_sklearn_baseline(X_train, y_train)
        probs = probs_sklearn
        model_name = "Sklearn"
    else:
        probs = probs_pyro
        model_name = "Pyro"

    df_eval = df_test.copy()
    df_eval["MVP_Prob"] = probs
    df_eval["Actual_MVP"] = y_test

    # Select top player per year
    top_preds = df_eval.loc[df_eval.groupby("Year")["MVP_Prob"].idxmax()]
    df_eval["Predicted_MVP"] = 0
    df_eval.loc[top_preds.index, "Predicted_MVP"] = 1

    # Accuracy
    correct_by_year = (top_preds["Actual_MVP"] == 1).sum()
    total_years = top_preds["Year"].nunique()
    print(f"\n{model_name} Correct MVP Predictions on Test Set: {correct_by_year}/{total_years} ({100 * correct_by_year / total_years:.2f}%)")

    # Top predictions for recent test years
    print(f"\nTop 5 {model_name} MVP Predictions for Recent Test Years:")
    recent_years = sorted(df_eval['Year'].unique())[-5:]
    
    for year in recent_years:
        year_data = df_eval[df_eval['Year'] == year].nlargest(5, 'MVP_Prob')
        print(f"\n{year}:")
        for _, row in year_data.iterrows():
            mvp_status = "**MVP**" if row['Actual_MVP'] == 1 else ""
            print(f"  {row['Player']}: {row['MVP_Prob']:.6f} {mvp_status}")

    return df_eval
# To run: 
df_results = run_pipeline_from_df(df)
df_results


from pandas.api.types import is_numeric_dtype, is_object_dtype
from sklearn.model_selection import train_test_split

# -------------------- Preprocessing --------------------

def load_and_preprocess_from_df(df):
    df = df.copy()
    df = df.dropna(subset=['Won_MVP'])

    y = df['Won_MVP'].astype(int).values
    exclude_cols = ['Player', 'Tm', 'Won_MVP', 'G']
    features = [col for col in df.columns if col not in exclude_cols]

    for col in features:
        if df[col].isnull().any():
            median = df[col].median()
            df[col] = df[col].fillna(median)

    bins_dict = {}
    X = []
    for col in features:
        if is_object_dtype(df[col].dtype):
            continue
        binned, bins = pd.qcut(df[col], q=10, labels=False, retbins=True, duplicates='drop')
        X.append(binned.astype(int).values)
        bins_dict[col] = bins

    X = np.stack(X, axis=1)
    return X, y, features, bins_dict

# -------------------- Train/Test Split by Year --------------------

def train_test_split_by_year(df, test_frac=0.2, seed=42):
    years = sorted(df['Year'].unique())
    train_years, test_years = train_test_split(years, test_size=test_frac, random_state=seed)
    return train_years, test_years

# -------------------- Grouping --------------------

def reshape_data_by_year(X, y, df, selected_years):
    X_grouped, y_grouped, year_player_indices = [], [], []
    for year in selected_years:
        mask = df['Year'] == year
        X_year = X[mask]
        y_year = y[mask]
        if y_year.sum() == 0:  # Skip if no MVP for this year
            continue
        X_grouped.append(X_year)
        y_grouped.append(np.argmax(y_year))  # Index of MVP
        year_player_indices.append(np.where(mask)[0])
    return X_grouped, y_grouped, year_player_indices

# -------------------- Pyro Per-Year Model --------------------

def build_per_year_model(X_grouped, feature_bins, alpha=1.0):
    def model(y_grouped):
        num_years = len(X_grouped)

        for year_idx in pyro.plate("years", num_years):
            X_year = torch.tensor(X_grouped[year_idx], dtype=torch.long)
            feature_log_probs = []
            for i, bins in enumerate(feature_bins):
                probs = pyro.param(f"feature_{i}_probs",
                                   torch.ones(bins) / bins,
                                   constraint=dist.constraints.simplex)
                vals = X_year[:, i]
                logp = torch.log(probs[vals])
                feature_log_probs.append(logp)
            total_log_probs = torch.sum(torch.stack(feature_log_probs, dim=1), dim=1)
            pyro.sample(f"y_{year_idx}", dist.Categorical(logits=total_log_probs),
                        obs=torch.tensor(y_grouped[year_idx]))
    return model

# -------------------- Training --------------------

def train_per_year_model(X_grouped, y_grouped, feature_bins, alpha=1.0):
    pyro.clear_param_store()
    model = build_per_year_model(X_grouped, feature_bins, alpha)
    svi = SVI(model, guide=lambda *args, **kwargs: None,
              optim=Adam({"lr": 0.01}), loss=Trace_ELBO())

    for step in range(1000):
        loss = svi.step(y_grouped)
        if step % 100 == 0:
            print(f"[{step}] Loss: {loss:.2f}")

# -------------------- Predict Per-Year --------------------

def predict_per_year(X_grouped, feature_bins):
    predictions = []
    for X_year in X_grouped:
        X_year = torch.tensor(X_year, dtype=torch.long)
        feature_log_probs = []
        for i, bins in enumerate(feature_bins):
            probs = pyro.param(f"feature_{i}_probs").detach()
            vals = X_year[:, i]
            logp = torch.log(probs[vals])
            feature_log_probs.append(logp)
        total_log_probs = torch.sum(torch.stack(feature_log_probs, dim=1), dim=1)
        probs = torch.softmax(total_log_probs, dim=0)
        predictions.append(probs.numpy())
    return predictions

# -------------------- Main Pipeline --------------------

def run_pipeline_from_df(df, test_frac=0.2):
    X, y, features, bins_dict = load_and_preprocess_from_df(df)
    feature_bins = [len(bins_dict[col]) - 1 for col in features if is_numeric_dtype(df[col])]

    train_years, test_years = train_test_split_by_year(df, test_frac)
    print(f"Train years: {sorted(train_years)}")
    print(f"Test years: {sorted(test_years)}")

    # Group by year
    X_train_grouped, y_train_grouped, train_indices = reshape_data_by_year(X, y, df, train_years)
    X_test_grouped, y_test_grouped, test_indices = reshape_data_by_year(X, y, df, test_years)

    # Train
    print("=== Training Model ===")
    train_per_year_model(X_train_grouped, y_train_grouped, feature_bins)

    # Predict on all years
    print("\n=== Predicting ===")
    probs_train = predict_per_year(X_train_grouped, feature_bins)
    probs_test = predict_per_year(X_test_grouped, feature_bins)

    df_eval = df.copy()
    df_eval["MVP_Prob"] = 0.0

    # Fill in predictions
    for probs, idx in zip(probs_train, train_indices):
        df_eval.loc[idx, "MVP_Prob"] = probs
    for probs, idx in zip(probs_test, test_indices):
        df_eval.loc[idx, "MVP_Prob"] = probs

    df_eval["Actual_MVP"] = y
    df_eval["Predicted_MVP"] = 0

    # Evaluate only on test set
    top_preds = df_eval[df_eval["Year"].isin(test_years)].loc[
        df_eval[df_eval["Year"].isin(test_years)].groupby("Year")["MVP_Prob"].idxmax()
    ]
    df_eval.loc[top_preds.index, "Predicted_MVP"] = 1

    correct = (top_preds["Actual_MVP"] == 1).sum()
    total = len(test_years)
    print(f"\nTest Set Accuracy: {correct}/{total} = {100 * correct / total:.2f}%")

    print(f"\nTop 5 MVP Predictions for Recent Years:")
    recent_years = test_years  # Last 5 years
    
    for year in recent_years:
        year_data = df_eval[df_eval['Year'] == year].nlargest(5, 'MVP_Prob')
        print(f"\n{year}:")
        for _, row in year_data.iterrows():
            mvp_status = "**MVP**" if row['Actual_MVP'] == 1 else ""
            print(f"  {row['Player']}: {row['MVP_Prob']:.6f} {mvp_status}")

    return df_eval

# -------------------- Run --------------------
df_results = run_pipeline_from_df(df)



